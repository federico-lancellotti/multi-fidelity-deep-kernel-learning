# Multi-Fidelity Deep Kernel Learning
This project proposes a Stochastic Variational Deep Kernel Learning method for the data-driven discovery of a low-dimensional representation of a dynamical system, from high-dimensional data at different levels of fidelity.
The framework is composed of multiple instances of a DKL model, each of them associated to a particular level of fidelity. Each instance includes an autoencoder, that compresses the high dimensional data (a video of the dynamical system) into a low-dimensional latent state space, and a latent dynamical model, that predicts the system evolution over time. 
The latent representation of the system and its dynamics is used to estimate the intrinsic dimensionality of the system and as a correction term for the higher levels of fidelity.

## Getting Started
These instructions will help you run the code on you local machine. Please be aware that the usage of a GPU is strongly advised, especially for the training of the model. Currently, only CUDA is supported for GPU-based training.
A config.yaml file is available to set the main parameters.

### Requirements
The code has been tested on Python 3.9.6 and 3.10.13.
The required libraries are collected in the requirements.txt file:
```
pip install -r requirements.txt
```

## Generate the dataset
The dataset can be generated by running the `generate_dataset.py` file. 

Please, be sure to select the desired environment and the desired number of episodes. The following are currently available:
- Pendulum (200 frames per episode);
- Acrobot (500 frames per episode);
- MountainCarContinuous (400 frames per episode).

The frames can be optionally cropped and an occlusion can be generated for specific levels of fidelity. It is sufficient to specify the portion of the cropping/occlusion and its position as a dictionary in the crop/occlusion parameter list.
An example, where for the second and third levels only 60% of the frame is selected, from the fourth quadrant:
```
crop: [{'portion': 1, 'pos': 4}, {'portion': 0.6, 'pos': 4}, {'portion': 1, 'pos': 4}, {'portion': 1, 'pos': 4}]
```
More info on this can be found in the documentation of the GenerateDataset class.

Two pickle files for each level of fidelity are eventually produced, one for training and the other for testing purposes, inside the Data folder.

## Train the model
The model can be initialized and trained by running the `main.py` file. Here you can also set `use_gpu = True` if you want to use your CUDA compatible GPU.

To initialize the model, you firstly need to instantiate a BuildModel object.
You can 
- add a level to the model with the method `add_level(level, latent_dim)`, by specifying the desired level of fidelity and the desired dimension of its latent space;
- train the level with the method `train_level(level, model_n)`, by specifying the desired level of fidelity and passing the sub-model returned when adding the respective level;
- evaluate the trained level with the method `eval_level(model_n, train_loader_n)`, by passing the trained sub-model and its data loader. By evaluating the model, the latent representations of the pair of frames (t-1,t) and (t,t+1) and of the dynamics of the system will be returned.

You can estimate the intrinsic dimension of the system with the function `estimate_ID(z_n, z_next_n, z_fwd_n)`, by passing the previously computed latent representations of the dynamical system.
You can then use the estimated ID as the `latent_dim` of the next level of fidelity.

The weights of the trained model will be saved in the folder Results/NameOfEnv/date_time.
By default, an ID.txt file is produced and saved inside the same folder. It contains the estimate of the intrinsic dimension of the system computed until the second to last fidelity level, namely the dimension of the latent space of the level of highest fidelity.

## Test the model
The trained model can be tested by running the `test.py` file. 

Please, make sure you are correctly specifying the `results_folder, weights_filename, ID` parameters inside the `config.yaml` file. 

To test the model, you should firstly instantiate a BuildModel object, specifying the `test=True` argument. 
You can then:
- add a level to the model with the method `add_level(level, latent_dim)`, by specifying the desired level of fidelity and the used dimension of its latent space;
- test the level with the method `test_level(level, model_n)`, by specifying the desired level of fidelity and passing the sub-model returned when adding the respective level;
- evaluate the trained level with the method `eval_level(model_n, train_loader_n)`, by passing the trained sub-model and its data loader. By evaluating the model, the latent representations of the pair of frames (t-1,t) and (t,t+1) and of the dynamics of the system will be returned.

By default, the rest of the `test.py` file produces:
- the plots of the true state variables, saved during the generation of the dataset;
- the plots of the latent variables of the level of highest fidelity;
- the plots of 50 random tuple of consecutive frames, with each tuple containing the actual frame at time t, its reconstruction and the predicted following frame.
The plots can be found inside the folder Results/NameOfEnv/date_time/plots.

## References
- https://github.com/nicob15/DeepKernelLearningOfDynamicalModels
- https://github.com/BoyuanChen/neural-state-variables